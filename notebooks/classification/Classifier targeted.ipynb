{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted Classifier Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sklearn\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"../../data/dataset.json\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mixed(instance):\n",
    "    return len(set(instance[\"labels\"].values())) >= 2\n",
    "\n",
    "len([x for x in data if is_mixed(x)]), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len([x for x in data if len(x[\"labels\"]) > 1]), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label2id = {\n",
    "    \"NEG\": 0,\n",
    "    \"NEU\": 1,\n",
    "    \"POS\": 2\n",
    "}\n",
    "\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "\n",
    "def build_auxiliary_sentence(target):\n",
    "    return target\n",
    "\n",
    "def build_df(data):\n",
    "    return pd.DataFrame([{\n",
    "        \"id\": x[\"id\"],\n",
    "        \"sentence\": x[\"titulo\"],\n",
    "        \"target\": target,\n",
    "        \"auxiliary_sentence\": build_auxiliary_sentence(target),\n",
    "        \"label\": label2id[label],\n",
    "    } for x in data for (target, label) in x[\"labels\"].items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien, está dentro de todo equilibrado\n",
    "\n",
    "## Train test split\n",
    "\n",
    "Lo hago sobre el dataset. No sería mejor hacerlo por oraciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, random_state=2021, stratify=[len(x[\"labels\"]) for x in data])\n",
    "\n",
    "train_data, dev_data = train_test_split(train_data, random_state=2021, stratify=[len(x[\"labels\"]) for x in train_data])\n",
    "\n",
    "len(train_data), len(dev_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Value, ClassLabel, Features\n",
    "\n",
    "train_df = build_df(train_data)\n",
    "dev_df = build_df(dev_data)\n",
    "test_df = build_df(test_data)\n",
    "\n",
    "features = Features({\n",
    "    'sentence': Value('string'),\n",
    "    'auxiliary_sentence': Value('string'),\n",
    "    'label': ClassLabel(num_classes=3, names=[\"neg\", \"neu\", \"pos\"])\n",
    "})\n",
    "\n",
    "columns = [\"sentence\", \"auxiliary_sentence\", \"label\"]\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[columns], features=features)\n",
    "dev_dataset = Dataset.from_pandas(dev_df[columns], features=features)\n",
    "test_dataset = Dataset.from_pandas(test_df[columns], features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, return_dict=True, num_labels=len(id2label)\n",
    ")\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.model_max_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], batch['auxiliary_sentence'], padding='max_length', truncation=True)\n",
    "\n",
    "batch_size = 16\n",
    "eval_batch_size = 8\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
    "dev_dataset = dev_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lens = [sum(x[\"attention_mask\"]) for x in train_dataset]\n",
    "\n",
    "\n",
    "plt.hist(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_dataset(dataset):\n",
    "    dataset = dataset.map(lambda x: {\"labels\": x[\"label\"]})\n",
    "    columns = ['input_ids', 'attention_mask', 'labels']\n",
    "    if 'token_type_ids' in dataset.features:\n",
    "        columns.append('token_type_ids')\n",
    "    dataset.set_format(type='torch', columns=columns)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = format_dataset(train_dataset)\n",
    "dev_dataset = format_dataset(dev_dataset)\n",
    "test_dataset = format_dataset(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred, id2label):\n",
    "    \"\"\"\n",
    "    Compute metrics for Trainer\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    ret = {}\n",
    "\n",
    "    f1s = []\n",
    "    precs = []\n",
    "    recalls = []\n",
    "\n",
    "    for i, cat in id2label.items():\n",
    "        cat_labels, cat_preds = labels == i, preds == i\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            cat_labels, cat_preds, average='binary', zero_division=0,\n",
    "        )\n",
    "\n",
    "        f1s.append(f1)\n",
    "        precs.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        ret[cat.lower()+\"_f1\"] = f1\n",
    "        ret[cat.lower()+\"_precision\"] = precision\n",
    "        ret[cat.lower()+\"_recall\"] = recall\n",
    "\n",
    "    _, _, micro_f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"micro\"\n",
    "    )\n",
    "    ret[\"micro_f1\"] = micro_f1\n",
    "    ret[\"macro_f1\"] = torch.Tensor(f1s).mean()\n",
    "    ret[\"macro_precision\"] = torch.Tensor(precs).mean()\n",
    "    ret[\"macro_recall\"] = torch.Tensor(recalls).mean()\n",
    "\n",
    "    ret[\"acc\"] = accuracy_score(labels, preds)\n",
    "\n",
    "    return ret\n",
    "\n",
    "epochs = 5\n",
    "warmup_proportion = 0.10\n",
    "total_steps = (epochs * len(train_dataset)) // batch_size\n",
    "warmup_steps = int(warmup_proportion * total_steps)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/',\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_eval=False,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=lambda x: compute_metrics(x, id2label=id2label),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def predict(sentence, target):\n",
    "    \"\"\"\n",
    "    Return most likely class for the sentence\n",
    "    \"\"\"\n",
    "    idx = torch.LongTensor(tokenizer.encode(sentence, build_auxiliary_sentence(target))).view(1, -1).to(device)\n",
    "    output = model(idx)\n",
    "    probs = F.softmax(output.logits, dim=1).view(-1)\n",
    "    probas = {id2label[i]:probs[i].item() for i in id2label}\n",
    "\n",
    "    return probas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Gran 2020 para Messi, pero bastante malo para Cristiano Ronaldo\"\n",
    "\n",
    "targets = [\"Messi\", \"Cristiano Ronaldo\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Alberto Fernández: \\\"El gobierno de Macri fue un desastre\\\"\"\n",
    "\n",
    "targets = [\"Alberto Fernández\", \"Macri\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Alberto Fernández: \\\"El gobierno de Xi Jin Ping fue un desastre\\\"\"\n",
    "\n",
    "targets = [\"Alberto Fernández\", \"Xi Jin Ping\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Romina Del Plá denunció el ajuste de Alberto Fernández\"\n",
    "\n",
    "targets = [\"Romina del Plá\", \"Alberto Fernández\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"El Gobierno mejora la compra de vacunas\"\n",
    "\n",
    "targets = [\"Gobierno\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Cristina Kirchner acusa al Gobierno de corrupción en la compra de vacunas\"\n",
    "\n",
    "targets = [\"Gobierno\", \"Cristina Kirchner\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"El duro comunicado de las empresas estadounidenses contra el Gobierno\"\n",
    "\n",
    "targets = [\"Gobierno\", \"empresas estadounidenses\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ultiman negociaciones para cerrar acuerdo en la Ciudad entre Vidal y Bullrich\"\n",
    "\n",
    "targets = [\"Vidal\", \"Bullrich\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\\\"Este Gobierno ha sido una lacra corrupta inmunda\\\" declaró Patricia Bullrich\"\n",
    "\n",
    "targets = [\"Gobierno\", \"Bullrich\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo ponemos adelante, sin embargo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo ponemos adelante, sin embargo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Patricia Bullrich: \\\"Este Gobierno ha sido una lacra corrupta inmunda\\\"\"\n",
    "\n",
    "targets = [\"Gobierno\", \"Bullrich\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Keiko: 'Pedro Castillo es un pésimo dirigente, pero por suerte este Gobierno es lo mejor que le ha pasado a Perú'\"\n",
    "\n",
    "targets = [\"Keiko\", \"Pedro Castillo\", \"Gobierno\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"El contundente mensaje de L-Gante contra Cristina Kirchner: \\\"A mí nadie me regaló nada\\\"\"\n",
    "\n",
    "targets = [\"L-Gante\", \"Cristina Kirchner\"]\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Causa Maradona: Dalma y Gianinna denunciaron que quieren que Matías Morla se vaya\"\n",
    "\n",
    "targets = [\"Dalma\", \"Gianinna\", \"Matías Morla\"]\n",
    "\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    print(predict(sentence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance in [x for x in test_data if is_mixed(x)]:\n",
    "    print(\"=\"*80)\n",
    "    print(instance[\"labels\"])\n",
    "    sentence = instance[\"titulo\"]\n",
    "    print(sentence)\n",
    "\n",
    "    targets = instance[\"labels\"]\n",
    "\n",
    "    for target, label in targets.items():\n",
    "        probas = pd.Series(predict(sentence, target))\n",
    "        predicted_label = probas.index[probas.argmax()]\n",
    "\n",
    "        if {predicted_label, label} == {\"NEG\", \"POS\"}:\n",
    "            print(target)\n",
    "            print(probas)\n",
    "        elif predicted_label != label:\n",
    "            print(target , f\" era {label}, predijo {predicted_label}\")\n",
    "        else:\n",
    "            print(target, \" OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c22e1f0f586e152b720d45074bb422dadc7ff3eb06c32e08d5ec49d7ee1ec582"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('3.8.5': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}